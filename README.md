# Регрессионная задача: README

## Основные выводы и результаты

### Очистка данных
- **Обработка пропущенных значений**:
  - Проверка пропусков с помощью `data.isnull().sum()`.
  - Небольшое количество пропущенных значений было заполнено средним значением для числовых колонок.

- **Обработка нечисловых данных**:
  - Нечисловые колонки определены с помощью `data.select_dtypes(exclude=[np.number])`.
  - Эти колонки закодированы в числовые значения с использованием `LabelEncoder`.

- **Обнаружение выбросов**:
  - Выбросы исследованы визуально с помощью box plot и гистограмм.
  - Выбросы не удалялись, чтобы сохранить потенциально важную информацию.

- **Стандартизация и масштабирование**:
  - Данные масштабированы с использованием `StandardScaler`, особенно для моделей, чувствительных к масштабу признаков.

### Исследовательский анализ данных (EDA)
- **Сводная статистика**:
  - Использован `data.describe()` для изучения ключевых статистик, таких как среднее, медиана, минимальное и максимальное значения для каждого признака.
  - Это позволило понять масштаб и вариабельность признаков.

- **Анализ пропущенных значений**:
  - Проверка пропусков с использованием `data.isnull().sum()`.
  - Визуализация пропущенных значений с помощью heatmap (`sns.heatmap(data.isnull())`) для выявления паттернов.
  - Пример графика:

    ```python
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.heatmap(data.isnull(), cbar=False, cmap="viridis")
    plt.title("Пропущенные значения")
    plt.show()
    ```

- **Распределения признаков**:
  - Построены гистограммы для числовых признаков с использованием `data.hist()` для изучения их распределения (например, нормальное, скошенное).
  - Пример графика:

    ```python
    data["danceability"].hist(bins=20, color='blue', alpha=0.7)
    plt.title("Распределение danceability")
    plt.xlabel("Значение")
    plt.ylabel("Частота")
    plt.show()
    ```

- **Корреляционный анализ**:
  - Построена корреляционная матрица с использованием `data.corr()` для определения связей между числовыми признаками и целевой переменной.
  - Визуализация корреляционной матрицы с помощью heatmap (`sns.heatmap(data.corr(), annot=True)`), чтобы выделить сильные положительные или отрицательные корреляции.
  - Пример графика:

    ```python
    corr = data.corr()
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Корреляционная матрица")
    plt.show()
    ```

- **Визуализация взаимосвязей**:
  - Использовались scatter plot (`sns.scatterplot`) для изучения связей между ключевыми признаками и целевой переменной (`popularity`).
  - Пример графика:

    ```python
    sns.scatterplot(x=data["danceability"], y=data["popularity"], alpha=0.6)
    plt.title("Взаимосвязь danceability и popularity")
    plt.xlabel("Danceability")
    plt.ylabel("Popularity")
    plt.show()
    ```

  - Box plot (`sns.boxplot`) применялся для анализа категориальных признаков (если таковые были) в отношении целевой переменной.

- **Основные выводы**:
  - Обнаружена сильная положительная корреляция между `danceability` и `popularity`.
  - Взаимодействие признаков (`danceability * energy`) было определено как потенциально важное на основе scatter plot.

### Инженерия признаков
- **Новые признаки**:
  - Создан новый признак (`new_feature = danceability * energy`), который улучшил производительность модели.
  - Корреляционный анализ показал, что новый признак имеет значимую положительную связь с целевой переменной.

- **Важность признаков**:
  - С использованием модели Random Forest определены наиболее значимые признаки:
    - `danceability`
    - `energy`
    - `valence`
    - `loudness`
    - Новый признак `new_feature`

### Эксперименты с моделями и производительность
- **Протестированные модели**:
  - Линейная регрессия: Базовая модель, плохо справляющаяся с нелинейными связями.
  - Решающие деревья: Захватили нелинейные связи, но склонны к переобучению.
  - Random Forest: Лучшая производительность, сбалансированная сложность и точность.
  - Нейронные сети: Гибкие, но требуют значительных вычислительных ресурсов и тщательной настройки.

- **Лучшая модель**:
  - **Random Forest Regressor** показала лучшую производительность:
    - MSE на обучающей выборке: **26.5074**
    - Оптимальные гиперпараметры:
      - `n_estimators`: 100
      - `max_depth`: 30
      - `max_features`: `'sqrt'`
      - `min_samples_leaf`: 3
      - `min_samples_split`: 6
      - `bootstrap`: False

### Итоговая производительность
- Оптимизированная модель Random Forest продемонстрировала отличные предсказательные способности, достигнув низкого MSE на обучении и стабильных результатов при кросс-валидации.
- Анализ важности признаков выявил несколько ключевых признаков, сильно влияющих на предсказания, предоставив ценные инсайты о данных.

## Структура файла EDA_S5.ipynb
1. **Очистка данных**:
   - Обработка пропусков, нечисловых данных, выбросов и стандартизация.
2. **Исследовательский анализ данных**:
   - Статистика, распределения, корреляции и визуализации.
3. **Инженерия признаков**:
   - Создание новых признаков и анализ их важности.
4. **Эксперименты с моделями**:
   - Сравнение линейной регрессии, решающих деревьев, Random Forest и нейронных сетей.
5. **Тонкая настройка гиперпараметров**:
   - Оптимизация Random Forest с использованием RandomizedSearchCV.
6. **Итоговая оценка**:
   - Анализ производительности оптимизированной модели и значимости признаков.

## Как запустить код
1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/AMekhrza/EDA_S5.git
   cd EDA_S5
  ```
2. Установите зависимости:
  ```bash
  pip install -r requirements.txt
  ```
3. Запустите `EDA_S5.ipynb` в Jupyter Notebook и следуйте разделам.
4. Используйте оптимизированную модель (`optimized_rf_model.pkl`) для предсказаний:
  ```python
  import joblib
  model = joblib.load('optimized_rf_model.pkl')
  predictions = model.predict(X_new)
  ```